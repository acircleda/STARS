---
title: "STARS and Stripes: Political and Institutional Factors that Affect Higher Education Sustainability Ratings"
output:
  bookdown::word_document2:
    reference_docx: "apa7template.docx"
    number_sections: false
bibliography: r_references.bib
csl: apa.csl
---

\newpage

```{r setup, include = FALSE}
library(tidyverse)
library(lmerTest)
library(flextable)
library(psych)
library(broom.mixed)
library(modelsummary)
load("data/project data.RData")

`%notin%` <- Negate(`%in%`)

# table set-up ----

flex <- function(data, title=NULL) {
  flextable(data) %>%
  set_table_properties(layout = "autofit", width = 1) %>%
  fontsize(size=10, part="all") %>%
  font(fontname="Times New Roman", part="all")
}

flex_lme <- function(data, title=NULL) {
  set_table_properties(data, layout = "autofit", width = 1) %>%
  fontsize(size=10, part="all") %>%
  font(fontname="Times New Roman", part="all")
}

corstars <-function(x, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
                    result=c("none", "html", "latex")){
  #Compute correlation matrix
  #require(Hmisc)
  x <- as.matrix(x)
  correlation_matrix<-Hmisc::rcorr(x, type=method[1])
  R <- correlation_matrix$r # Matrix of correlation coefficients
  p <- correlation_matrix$P # Matrix of p-value 
  
  ## Define notions for significance levels; spacing is important.
  mystars <- ifelse(p < .0001, "****", ifelse(p < .001, "*** ", ifelse(p < .01, "**  ", ifelse(p < .05, "*   ", "    "))))
  
  ## truncate the correlation matrix to two decimal
  R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
  
  ## build a new matrix that includes the correlations with their appropriate stars
  Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
  diag(Rnew) <- paste(diag(R), " ", sep="")
  rownames(Rnew) <- colnames(x)
  colnames(Rnew) <- paste(colnames(x), "", sep="")
  
  ## remove upper triangle of correlation matrix
  if(removeTriangle[1]=="upper"){
    Rnew <- as.matrix(Rnew)
    Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
    Rnew <- as.data.frame(Rnew)
  }
  
  ## remove lower triangle of correlation matrix
  else if(removeTriangle[1]=="lower"){
    Rnew <- as.matrix(Rnew)
    Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
    Rnew <- as.data.frame(Rnew)
  }
  
  ## remove last column and return the correlation matrix
  Rnew <- cbind(Rnew[1:length(Rnew)-1])
  if (result[1]=="none") return(Rnew)
  else{
    if(result[1]=="html") print(xtable(Rnew), type="html")
    else print(xtable(Rnew), type="latex") 
    
  }
}


# auto add adjusted icc to tables ----

glance_custom.lmerMod <- function(x) {
  r2c <-  performance::r2(x)[["R2_conditional"]][["Conditional R2"]]
  r2m <- performance::r2(x)[["R2_marginal"]][["Marginal R2"]]
    
 data.frame(`ICC (Adjusted)` = performance::icc(x)$ICC_adjusted,
            `ICC (Cond.)` = performance::icc(x)$ICC_conditional,
            `Pseudo R2 (Marginal)` = r2m,
            `Pseudo R2 (Cond.)` = r2c)
}

PVE <- function(model1, model2) {

  vc1 <- as.data.frame(VarCorr(model1))
  vc2 <- as.data.frame(VarCorr(model2))
  PVE.2 <- ((vc1$vcov[1]- vc2$vcov[1]) / (vc1$vcov[1]))
  PVE.1 <- ((((summary(model1)$sigma)^2 - 
                (summary(model2)$sigma)^2) / (summary(model1)$sigma)^2))
  cat("Proportion of variance explained at level-1 = ", round(PVE.1,3), "\n")
  cat("Proportion of variance explained at level-2 = ", round(PVE.2, 3))
}

# plausible values ----
pv <- function (model, variable) {

  intercept <- broom.mixed::tidy(model) %>%
    filter(term == variable) %>%
    pull(estimate)
  
  sd <- broom.mixed::tidy(model) %>%
    filter(term == paste0("sd__", variable)) %>%
    pull(estimate)
  
  pv_lower <- round(intercept-(1.96*sd),2)
  pv_upper <- round(intercept+(1.96*sd),2)
  
  paste0("[",pv_lower,", ", pv_upper, "]")
}


```

```{r analysis-preferences, message=FALSE, warning=FALSE, include=FALSE}
# Seed for random number generation
set.seed(667)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

  Higher education institutions play a vital role in society, not only for ensuring a highly educated population but also for solving real-world issues. One such issue is environmental sustainability. Working towards a sustainable society fits within higher education's three-pronged mission of education, research, and public service [@Medlin2008]. That is, in working towards sustainable development goals, colleges and universities can ensure students come away with an understanding of the environment and their impact on it. Post-secondary institutions not only research the natural, technological, and psychological aspects of sustainability but also serve as role models that embody sustainability within their management and work with the community.
  
  Colleges and universities have made numerous commitments to environmental sustainability starting as early as the 1970s (see @Wright2002 for a detailed overview). Among the first commitments was the *Tblisi Declaration* (1978), which promoted higher education institutionsâ€™ roles in fostering environmental education. The *Talloires Declaration* (1990) garnered support from institutional administrators, who committed their institutions to various sustainability practices on their campuses (Wright, 2002). A more recent commitment focused on addressing the climate crisis: the American College and University Presidents Climate Commitment, which focused on mitigating greenhouse gasses (GHGs) and achieving carbon neutrality [@Medlin2008].
  
  Concomitant with the rise in campus sustainability efforts has been the development of higher education sustainability assessments (HESAs), such as the *College Sustainability Report Card* [@SEI], *Green Colleges* [@GreenSchools], and the *Sustainability Tracking, Assessment & Rating System* [STARS; @STARS]. @bullock2016comprehensiveness offered a content validity study of HESAs, comparing nine HESAs against indicators on a higher-education version of the Global Reporting Initiative, an international industry standard for assessing issues such as climate change. @bullock2016comprehensiveness matched GRI criteria to HESA criteria and calculated percentages that indicated each HESA's coverage of the criteria. Two HESAs came out ahead of all others: the Pacific Sustainability Index (PSI), which is no longer active, and STARS, which is active, ongoing, and the most ubiquitous HESA available. Indeed, most institutional sustainability websites proudly display their STARS rating.
  
  STARS was developed by the Association for the Advancement of Sustainability in Higher Education (AASHE) and encompasses the full range of higher education institution types, from community colleges to doctoral universities. Each institution reports data on five key categories: Academics, Engagement, Operations, Planning & Administration (PA), and Innovation & Leadership. Using the STARS Reporting Tool, institutions input data in subcategories of each dimension, scores on which are averaged across subcategories, and then averaged across categories to reach a final score, ranging from 0-100. Based on their score, institutions also receive a recognition badge, running from bronze, silver, gold, and platinum. STARS is currently one of the most comprehensive HESA available [@bullock2016comprehensiveness; @alghamdi2017assessment].
  
  The STARS ratings can help give a cross-sectional snapshot into an institution's engagement in sustainability. What is more, it can also allow comparisons across institutions and can give insights into institutional differences in sustainability practices. Whereas most prior research has focused on the development of higher education policy [@grindsted2011sustainable; @sibbel2009pathways], sustainability in the curriculum [@hill2018integrating; @leal2018role; @TEJEDOR201829], and sustainability assessments [@sustanassess2012yarime; @BERZOSA2017812; @Ramos2013], there are few studies that look at the factors that influence an institution's engagement and success in sustainability. The present study aims to address this gap by analyzing all STARS-rated institutions to help understand what factors may influence institutional success in sustainability as judged by a high STARS rating. In particular, this study uses stakeholder theory to quantitatively assess how cultural, political, and financial forces affect an institution's sustainability rating.


# Literature Review
  
  The impetus for the development of HESAs springs from the history of higher education institutions' signing of various sustainability declarations. They serve as an internal measure of progress to meet the often lofty goals agreed upon in these declarations. These internal measures allow institutions to track progress, cite successes, and identify weaknesses. In addition, it allows for cross-institutional comparisons and benchmarking [@shriberg2002institutional].
  
  However, not all HESAs are made equal. Owing to its complex nature, different assessments measure or emphasize different aspects of sustainability. @shriberg2002institutional argues that an effective HESA should embody five important characteristics: identify important issues; allow for simple numeric comparisons but allow for institutional complexity; emphasize sustainability holistically rather than solely about energy efficiency; include measures of "mission, rewards, incentives and other process-oriented outcomes"; and is comprehensive (p. 155).
  
  What @shriberg2002institutional describes is the *ideal* tool. Few, if any, HESAs come close to embodying most of these ideals. As argued earlier, one of the strongest contenders for ideal HESA is the *Sustainability Tracking, Assessment & Rating System* [STARS; @STARS; @bullock2016comprehensiveness]. STARS began development by the Association for the Advancement of Sustainability in Higher Education (ASHE) in 2006, releasing its first official version in 2010. Since then, it has gone through several changes and is currently on version 2.2 of its assessment [@starshistory].
  
  STARS offers a comprehensive self-report assessment that encompasses sustainability domains. Academics focus on curricular sources of sustainability, including courses, programming, campus living, as well as research and scholarship. The category of Engagement focuses on the inclusion of sustainability in student and employee orientation, student life, professional development, outreach, community service, and public policy. Operations include GHG emissions, building design, resource usage, sustainable dining, transportation, waste, and grounds maintenance. Planning and Administration involves various aspects of sustainability such as planning, diversity and equity, sustainable investment, and employee well-being Finally, Innovation and Leadership allows for additional actions that are deemed above and beyond those already measured. These include actions such as third-party verification of data before submitting to STARS, exemplary sustainability initiatives, or innovative programs [@starstech].
  
  The comprehensiveness and flexibility of STARS is certainly a strength. Still, @STARSCanada points out that data collection before submission to STARS requires multiple campus offices and can be intensive. They hypothesize that those who do complete STARS must "have some degree of commitment to sustainability" (p. 261). To date, there are ratings for a total of 363 institutions across 46 states and the District of Columbia. This figure does not account for international institutions.
  
  STARS is a relatively new HESA, and as a result, only a handful of scholarly articles have utilized its data. For example, @murphy used STARS data in a case study of Evergreen State College for their Master's thesis. @qingqing, for their dissertation, uses STARS and a case study design to evaluate four institutions, each one representing a major Carnegie classifications (e.g. Doctoral, Master's, Baccalaureate, Associate's).
  
  Fewer peer-reviewed research articles have been published using STARS. @urbanski2015measuring appears to be the only authors who have examined STARS data in its entirety. They reviewed the STARS 2.0 data in-depth, drilling down to individual category scores. They found that most institutions have a narrower view of sustainability as a representation of the environment rather than its more complex focus on the interconnections between the environment, economics, and society. This is evidenced by the fact that most institutions focused on the green aspects of the measures "unless otherwise prompted by language in the STARS Technical Manual" [@urbanski2015measuring]. The authors also found that, while STARS is primarily a U.S.-based assessment, it is gaining popularity in other countries, with the inclusion of data from Canada, Mexico, and other countries. Other findings from this early dive into the data show that Doctoral institutions are heavily represented in the STARS data and that Carnegie status has a relationship to STARS ratings, with Doctoral and Baccalaureate institutions earning the highest rating and Associate's and Master's earning the lowest. Finally, most institutions struggled to earn points in their rating in areas relating to climate change, indicating institutions need more focus on mitigating GHGs and working towards carbon neutrality.
  
  Despite being a U.S. dominated tool, Canadian institutions using STARS have more representation in the published literature. For example, @STARSCanada utilized document analysis to qualitatively investigate sustainability policies from Canadian STARS-rated institutions. Echoing @urbanski2015measuring, they found that these institutions often used the broad definition of sustainability as the intersection of social, environmental, and economic concerns, though some universities had a specific focus on only the environment. In addition, the authors saw most institutional focuses on sustainability through facilities services, "with little emphasis on educating students in the classroom" (p. 275).
  
  To date, there have only been case studies and descriptive studies of STARS. There have been no quantitative studies that have attempted to assess the relationship institutional and politcal factors may have on STARS ratings. One of the purposes of the present study is to fill this gap. The present research attempts to model STARS' relationships to key variables that may influence institutional sustainability by using data pulled from secondary data sources following a stakeholder theory sustainability management framework.
  

# Conceptual Framework

  Understanding how sustainability develops within an organization has long been explained using stakeholder theory. Stakeholder theory is a management theory in which "Attention to the interests and well-being of those who can assist or hinder the achievement of the organization's objectives is the central admonition of the theory" [@phillips2003stakeholder, p. 481]. The term "stakeholder" can take on multiple meanings depending on organizational context or philosophical perspective, but in general, it represents any entity that affects or is affected by an organization. An example of this could be shareholders, creditors, and government [@elijido2007applying].
  
  According to @horisch2014applying, stakeholder theory is the most common theory applied to research on sustainability management. @shriberg2002sustainability identifies a number of stakeholders specific to sustainability management in higher education. Some of these stakeholders include "trustees, administrators, government, faculty, alumni and students" (p. 25). Interestingly, the role of individual campus "champions" - those dedicated to some environmental cause, are often a powerful source of institutional sustainability. Shriberg's (2002b) research found that stakeholders, especially students and faculty, were indeed important and had high correlations to sustainability outcomes. Specifically, @shriberg2002sustainability indicated that the strongest stakeholders, termed "drivers" of sustainability were, in decreasing order: faculty, government, institutional administrators, the institutional president, activists groups, donors, the labor market, and alumni (p. 137).
  
  In addition to stakeholders, @shriberg2002sustainability also analyzed other institutional factors that may influence sustainability, including political orientation of campus. It was found that "progressive" and "liberal" political orientations had moderate correlations to sustainability. While the focus of this research was on the internal stakeholders, the author notes that "Outside forces â€“ such as governmental agencies and leaders as well as the local community â€“ can also help drive sustainability efforts by influencing this committed core of individuals and institutional leaders" (p. 94).
  
  Much of the research on higher education sustainability has been on internal stakeholders, and few have taken an outward-looking approach to assess the role of the state and local communities. The state government can be a source of influence on higher education sustainability through any number of means, including policy development or funding. @heistate writes that the ways in which states can influence higher education are "largely through three mechanisms: planning and coordination, budgetary appropriations and the allocation of resources among institutions and sectors, and administrative regulation and control (p. 21). @bramwell2011governance draws on political economy to explain that the state can intervene to either prioritize or deprioritize environmental goals. While Bramwell was referring to "state" in the sense of "national government", applied to the United States' federal model, with vested power in the individual states also makes sense. How can a state affect sustainability? According to Bramwell, a state "may be in a position to offer incentives for actors to alter their behaviour so as to promote sustainability, or to impose requirements on actors to do this". A higher education institution could no doubt be one of these "actors". Indeed, an example of this is the Tennessee Higher Education Sustainability Initiative, which supports sustainability at institutions across the state of Tennessee [@tnhesi].
  
  Just as the state government may serve as a stakeholder that affects higher education sustainability initiatives, so too may the local community. Yet, little research in higher education considers the local community as a stakeholder. Ignoring the local community ignores the very location and culture in which an institution is based. In fact, the local community-institution relationship is very prevalent at the community college level, though not limited only to community colleges [@heilocal]. The relationship between the local community and an institution is dialectical: students, faculty, staff live in the community and shape the community culture; students, faculty, staff also shape the institutional culture. Depending on the percentage of students, faculty, staff from out-of-state, this culture may be either homogeneous or heterogeneous. @50shades shares a similar sentiment: graduates are influenced by the institutional culture and go on to influence the local/regional government. Likewise, "the general culture of a given provincial population may influence the political mandate there, which may in turn influence the direction of ... institutional policies" (p. 93).
  
  The current research uses stakeholder theory of sustainability management in higher education to assess which influences significantly affect sustainability ratings as measured through STARS. It focuses on top-down stakeholders (e.g. government) and bottom-up stakeholders (e.g. the local community). Because this research uses secondary data and, as such, does not measure stakeholders directly, proxies for these stakeholders will be used (as outlined below). This research aims to answer the following research questions:
  
  1. Is there a relationship between local political attitudes and institutional sustainability ratings?
  
  2. Is there a relationship between local environmental beliefs and institutional sustainability ratings?
  
  3. Is there a relationship between state political attitudes and institutional sustainability ratings?
  
  4. Is there a relationship between state environmental policies and institutional sustainability ratings?


# Methods

This research uses multilevel modeling to explore the relationship local and state stakeholders may have on institutional sustainability ratings. In total, 363 institutions are included in this study, representing 46 states and the District of Colombia (see Table \@ref(tab:instnum)).

## Data

Descriptive statistics for all variables can be found in Table \@ref(tab:descstats) (continuous variables) and Table \@ref(tab:descstats2) (categorical variables). The following paragraphs outline key variables used in this study, with variable names in parentheses.

*Dependent variable (STARS rating)*. The dependent variable is the overall STARS rating for each institution, a continuous value ranging from 0 to 1 (original metric: 0%-100%). The overall rating is calculated by STARS as the average across Engagement, Operations, Planning & Administration, Academics, and Innovation & Leadership scores. Each of those scores is a percentage based on the number of points scored out of the number of points available in each category [see @starstech]. This data comes from the entire population of STARS-rated institutions, accessed through the STARS benchmarking tool. Data was the most recent available as of September 2020. Figure \@ref(fig:starsboxplot) shows the distribution of STARS scores by state. Figure \@ref(fig:map) shows a map of all STARS-rated institutions included in the present study. The STARS ratings include ratings from a mix of STARS versions 2.0, 2.1, and 2.2, all of which are on the same scale.

### Level-1 Independent Variables

*County-level votes for the Democratic presidential candidate, 2012-2016 average (Vote percentage)*. This data comes from the @mitvotes. Vote percentage is a local variable that represents community stakeholders' political attitudes, and by extension, their influence on institutional culture. The vote percentage is based on election results for the county in which an institution is located. A vote percentage over 50% indicates the majority of voters chose a Democratic candidate, which suggests a more liberal constituency. Conversely, votes under 50% suggest a more Republican- or conservative-leaning county. The variable itself is continuous and ranges from 0-1. 

It is often recommended to center variables in a multilevel model, particularly centering on the group- or grand-mean [@enders2007centering]. Instead, centering at a theoretical value of .5 (50%) was used. This will make the intercept interpreted as the average STARS rating of an institution in a county that voted 50% Democrat. 

<!-- While this is theoretically possible, it was not observed in the data. The lowest county-level average vote for a Democratic presidential candidate observed in the sample data was 16% (among all US counties, the lowest was 3%). Nevertheless, the original metric at 0% Democratic votes serves as a useful baseline. -->

```{r message=FALSE, warning=FALSE, include=FALSE}
#center vote_pct at 50%
data <- data %>%
  mutate(vote_pct_dem = vote_pct_dem-.5)
```


  This variable is included for two primary reasons. First, it represents the political leanings of the communities in which institutions are situated. These communities are a complex mix of stakeholders who play direct or indirect roles in shaping the campus culture and, thus, cultural investment in sustainability. Some work for the institution (as faculty or staff), some attend the institution (as students), while others make up the economic (e.g., shops) and cultural institutions (e.g., churches) that surround the institutions, having some bearing on the campus culture.

  Second, partisan representation at the local level is hypothesized to play a role in sustainability ratings. Research has shown that Republican and conservative-leaning individuals are less likely to support environmental policies [@shriberg2002sustainability], and less likely to believe the scientific consensus of anthropogenic climate change [@YaleMaps]. These beliefs, however, are not monolithic and are subject to regional and geographic variation, which indicates they serve as an appropriate institutional level variable.
  
<!-- Note: the following variable was eventually removed from the model so I don't know if this is needed -->

<!-- Good point. However, since one of the research questions concerns local env beliefs I think we should keep it (or drop that RQ) -->

*Global Warming Agreement*. This county-level variable is based on the estimated percentage of adults who think global warming is mostly caused by human activities. The data comes from the 2018 version of the Yale Climate Opinion Maps [@YaleMaps] and represents the environmental stance of the community in which an institution is located. The variable is continuous, ranging from 0 to 1 and will be entered into the model centered at .5, following the same logic as the county-level votes. For conceptual purposes, "global warming" will be referred to by the broader phrase "climate change".

As mentioned above, partisan affiliation only *suggests* an environmental perspective. This variable allows a more accurate measure of environmental perspective by allowing belief in anthropogenic climate change to serve as a proxy.


### Level-1 Control Variables

*Carnegie classification (Carnegie)*. This is a dummy-coded variable for Doctoral, Master's, Baccalaureate, and other institutions (e.g., Associates, Special Focus), derived from IPEDS, 2019. Doctoral institutions serve as the reference group.

*Institutional Control (Control)*. This is a binary variable that indicates whether an institution is Public (*n* = 202) or Private (*n* = 158). This controls not only for the type of institution but also accounts for institutions that receive state higher education funding. Public institutions serve as the reference group.

*Average Revenue from Tuition and Fees (Revenue)*. This is the eight-year average for each institution, derived from IPEDS 2010-2018, for both public and private institutions. This variable controls for both spending and institutional size. It has been scaled (revenue divided by 1,000,000) for ease of interpretation. It will be entered into the model as a grand-mean centered variable.

*Mean Percent of Out-of-State Undergraduate Students (Out-of-State)*. This variable is included to control for the fact that, while communities may influence an institution, if the composition of the on-campus community is vastly different from the off-campus community, this influence may be weakened. This variable is derived from IPEDS data on percent of first-time out-of-state undergraduates, 2012-2018 (data was only available for this range). It will be entered in as a grand-mean centered variable.

  
### Level-2 Independent Variables

*Percentage of State Legislature that is Democratic (State Leg. Dem)*. This variable indicates, over the past 10 years, what percentage of each state's legislature were Democrats. This period accounts for the rise in sustainability interest and time it takes to implement sustainability policies. The percentage indicates whether a conservative or liberal government may have influenced institutional governance. This data comes from the National Conference on State Legislatures' (NCSL) State Partisan Composition Reports (2010-2020; [@stateleg]). This variable will be grand-mean centered.

The NCSL reports do not contain relevant data for Nebraska or Washington, D.C. Nebraska has a unicameral government (state senate only) with no formal party alignments. However, members of the government are party-affiliated. Data for Nebraska come from Ballotpedia (2020) and are taken from compositions following 2014, 2016, and 2018 elections, the only dates for which partisan data were available. For Washington, D.C., the legislative body is the city council, of which there is a Chairperson, three at large representatives, and representatives for each of eight wards. The number of Democratic representatives divided by the total number of representatives from 2010-2020 (as of October) were included [@dcwiki].


*Renewable Energy Targets (RPS targets)*. Many states have mandated or voluntary Renewable Portfolio Standard (RPS) targets, which set specific amounts of energy that must come from renewable sources. This variable includes the state target, 0-1 (representing 0%-100%), from the NCLS "State Renewable Portfolio Standard and Goals" (2020). This percentage, and having an RPS target in general, serves as a proxy for a state's commitment to sustainability. Note that target years may differ and range from 2015 to 2045 or later. For states with targets that increase by date, the latest date was included. For states with targets based on utility type (i.e., investor-owned, municipal), an average was taken. For states with no targets, 0% was used.

<!-- *State Higher Education Funding*. This variable consists of the eight-year average of per-state funding across public and private higher education institutions for the years 2010-2018. The data comes from the State Higher Education Executive Officers Association. -->

<!-- **Other variables under consideration** -->

<!--   * State % of democratic votes in 2016 election - grand mean centered? -->
<!--   * state average public opinion of climate change - grand mean centered? -->
  
```{r instnum, echo=FALSE, message=FALSE, warning=FALSE, tab.cap="Number of Institutions by State and Carnegie Classification"}
data %>%
  janitor::tabyl(state, carnegie) %>%
  janitor::adorn_totals(c("row", "col")) %>%
  rename("State" = "state") %>%
    flex()

```

```{r descstats, echo=FALSE, message=FALSE, warning=FALSE, tab.cap="Descriptive Statistics for Continuous Variables"}
data %>%
  select(stars_score, vote_pct_dem, climate_chg_agree,
         state_leg_pct_dem, rps_target, revenue_mil, out_of_state) %>%
describe() %>%
  as.data.frame() %>%
  #rownames_to_column("Variable") %>%
  select(mean, sd, min, max, skew, kurtosis) %>%
  slice(-1) %>%
  mutate(across(where(is.numeric),round,2)) %>%
  add_column(data.frame(Variable=c("STARS Rating",
                "Vote % (Dem)",
                "Climate Change Agree.",
                "State Legislauture (% Dem)",
                "State RPS Targets",
                "Revenue (in millions)",
                "% Out of State Stud.")), 
             .before=1) %>%
  flex()
```

```{r descstats2, echo=FALSE, message=FALSE, warning=FALSE, tab.cap="Descriptive Statistics for Categorical Variables"}
data %>%
  mutate(carnegie = factor(carnegie, levels=c(
    "Doctoral", "Master's", "Baccalaureate", "Other" 
  ))) %>%
  janitor::tabyl(carnegie, control) %>%
  left_join(
    data %>% group_by(carnegie) %>%
      summarize(`Average STARS Rating` = round(mean(stars_score),2))
  ) %>% flex() %>%
  footnote(i=4, j=1, value=as_paragraph(("90% community colleges; 10% private special focus institutions")))
```
  


```{r starsboxplot, fig.cap="Distribution of STARS Ratings by State", fig.height=6.5, message=TRUE, warning=FALSE, echo=FALSE}
data %>%
  ggplot(aes(stars_score, y=reorder(state, stars_score))) +
  geom_boxplot()+
  ylab("State")+
  xlab("STARS Rating")
  #ggtitle("Distribution of STARS Ratings by State")
```


```{r map, echo=FALSE, fig.cap="Map of STARS-rated Institutions", message=FALSE, warning=FALSE}
#map
latlong <- read.csv("data/counties.CSV", header=TRUE, stringsAsFactors=FALSE) %>%
  mutate(GEOID = as.numeric(GEOID))

map_data <- latlong %>%
  left_join(data, by=c("GEOID" = "fips_county_code_hd2019")) %>%
  drop_na(institution)

library(sf)
points <- st_as_sf(map_data, coords = c("INTPTLONG", "INTPTLAT"))

library("rnaturalearth")
library("rnaturalearthdata")
library(maps)

library(USAboundaries) 

USA <- us_boundaries()
counties <- st_as_sf(points, map("county", plot = FALSE, fill = TRUE))

ggplot(data = filter(USA, name != "Hawaii" & name != "Alaska"),
       fill="white") +
  geom_sf()+
  #geom_sf(data = counties, fill = NA, color = "grey80", alpha=.6)+
  geom_count(data = map_data, aes(x = INTPTLONG, y = INTPTLAT), alpha=.4)+
  theme_classic()+
  #ggtitle("Map of STARS-rated Institutions")+
  theme(axis.title = element_blank())+
  labs(fill = "Number of Institutions")+
  scale_y_continuous(limits=c(25,50))+
  theme_void()
```


## Analysis

The analysis used for this study views the data as hierarchical, with institutions nested within states. Therefore, a hierarchical linear model (i.e., multilevel model) with restricted maximum likelihood estimation was employed. Besides helping delineate between level-1 (local, county-level) effects and level-2 (state) effects, a multilevel model can reduce Type I errors and have less biased parameter estimates, as it takes into account violations of independence caused by nested data [@peugh]. In addition, the multilevel model allows us to simultaneously examine the relationship STARS ratings have with institutional and state-level characteristics.

An initial analysis of scatterplot matrices shows no major violations of linear relationships with the dependent variable, overall STARS score  (Figure \@ref(fig:scatterplotmatrix)). Examination of the correlation matrix indicates all continuous predictors have low to moderate positive, significant correlations to STARS score (Table \@ref(tab:cormatrix)). However, the correlation between vote percentage and agreement with climate change is .90, indicating a very strong association and possible multicollinearity (see Table \@ref(fig:cordemcc)). An inspection of variance inflation factors (VIF) showed VIFs greater than 5, indicating a possible violation of multicollinearity. It was reasoned that because climate change agreement is correlated so closely with Democratic vote percentage, both should not be modeled together. Democratic vote percentage measures political stance, which is more directly tied with local, state, and national politics than agreement with climate change. In addition, the variable itself is an aggregate based on a larger sample than the climate survey. No other variables indicated problems with multicollinearity.

Analyses were conducted using `R` 4.0.2 [@r]. Multilevel models were estimated using the`lmerTest` 3.1.3 [@lmertest] package. `lmerTest` p-values are calculated using Satterthwaite denominator degrees of freedom. Standardized coefficients and confidence intervals were estimated using the `effectsize` 0.3.3 [@effectsize] package.


```{r echo=FALSE, message=FALSE, warning=FALSE}
model_vif <- lmerTest::lmer(stars_score ~ 1 + vote_pct_dem + climate_chg_agree + (1 | state),
                          data=data, REML = T)
performance::check_collinearity(model_vif)
```



```{r scatterplotmatrix, echo=FALSE, fig.cap="Scatterplot Matrix of Continuous Variables", message=FALSE, warning=FALSE}
pairs(data %>% ungroup() %>%
  select(stars_score, vote_pct_dem, climate_chg_agree, out_of_state, state_leg_pct_dem, rps_target, revenue_mil),
  panel = panel.smooth)
```

```{r cormatrix, tab.cap="Correlation Between Continuous Predictors", message=FALSE, warning=FALSE, echo=FALSE}
data %>% ungroup() %>%
  select(stars_score, vote_pct_dem, climate_chg_agree,
         state_leg_pct_dem, rps_target, revenue_mil, out_of_state) %>%
  corstars(method="pearson") %>%
   add_column(data.frame(Variable=c("STARS Rating",
                "Vote % (Dem)",
                "Climate Change Agree.",
                "State Legislauture (% Dem)",
                "State RPS Targets",
                "Revenue (in millions)",
                "% Out of State Stud.")), 
             .before=1) %>%
  flex()
```
```{r cordemcc, fig.cap="County-Level Percentage of Climate Change Agreement", message=FALSE, warning=FALSE, echo=FALSE}
data %>%
  ggplot()+
  geom_point(aes(x=vote_pct_dem, y=climate_chg_agree))+
  xlab("Average County-Level Percentage of Votes for Democratic Candidate, 2012-2016")+
  ylab("County-Level Percentage of Climate Change Agreement")
```


# Results

```{r include=FALSE}
model_0 <- lmerTest::lmer(stars_score ~ 1 + (1 | state),
                          data=data, REML = T)
```


All model results are listed in Table \@ref(tab:models). The unconditional model (Model 0) contains the outcome variable (STARS score), no predictor variables, and a random effect for state. This model is useful for partitioning the variability in STARS ratings into within-state and between-state effects. This can be quantitatively represented by examining the ratio of variances through the intraclass correlation. The intraclass correlation for Model 0 was .059. In other words, 6% of the variance in STARS ratings can be attributed to differences between states. 

```{r include=FALSE}
#options(scipen = 999)

model_1 <- lmerTest::lmer(stars_score ~ 1 + vote_pct_dem +(1 | state),
                          data=data, REML = T)

model_2 <- lmerTest::lmer(stars_score ~ 1 + vote_pct_dem + 
                            carnegie + control + revenue_mil_c +
                            out_of_state_c + (1 | state),
                          data=data, REML = T)
```

  Model 1 introduces the main variable of interest, Democratic vote percentage. A likelihood ratio test^[All likelihood ratio tests used deviance statistics from models refit using full maximum likelihood estimation.] between Model 0 and Model 1 indicated Model 1 was an improvement in fit, $\chi^2(1)=18.376,\, p<.001$. Results from Model 1 indicate that Democratic vote percentage explained .02% of the variance in STARS ratings within states (i.e., level-1). The parameter estimate for vote percentage is significantly related to STARS rating: $\beta=.23\,(p<.001)$. These results suggest that for every one-percentage point increase in vote percentage, STARS score increased by .23 percentage points. However, this changes once important covariates enter Model 2.


```{r include=FALSE}
anova(model_0, model_1, refit=T)
PVE(model_0, model_1)

anova(model_1, model_2, refit=T)
PVE(model_0, model_2)
```

  Next, institution-level control variables (i.e., Carnegie classification, institutional control, revenue, percentage of out-of-state students) are entered into the model. Model 2 improves the fit compared to Model 1, $\chi^2(6)=73.215,\, p<.001$. The estimate for vote percentage is now smaller and non-significant, $\beta=.11, \,p=.050$. Master's and other Carnegie classifications are negative and significant. This indicates that Doctoral institutions have higher STARS scores than Master's and other Carnegie classifications. Private institutional control was also negative and significant, indicating Public institutions have higher STARS ratings than otherwise similar Private institutions. Revenue was another significant covariate, indicating that for every $1 million dollar increase in revenue there is a .01 percentage point increase in STARS rating. This may seem like a small increase, but as revenue can vary by tens or hundreds of millions of dollars between institutions, this value can quickly grow. Percentage of out of state students was also significant, meaning a one percentage point increase above average is associated with a .14 percentage point increase in STARS ratings.
  
  Compared to the unconditional model, the level-1 predictors explain 21% of variance in STARS rating within states; in addition, they explain 7.5% of 6% of variability in STARS ratings between states. 
  
```{r include=FALSE}
model_3 <- lmerTest::lmer(stars_score ~ 1 + vote_pct_dem + 
                            carnegie + control + revenue_mil_c +
                            out_of_state_c + (1 | state) +
                            rps_target + state_leg_pct_dem_c,
                          data=data, REML = T)




anova(model_2, model_3, refit=T)
PVE(model_0, model_3)
```
  
  
  A final model introducing state-level variables (i.e., RPS target, percentage of state legislature that is Democrat) was estimated next. Model fit was further improved by this model, $\chi^2(2)=18.976, \, p<.001$. The inclusion of these two state-level predictors, explained 90% of the 6% of  variation in STARS ratings between states. This suggests that nearly all of the variability in STARS ratings between states was accounted for in this model. A global pseudo-$R^2$ [@peugh], calculated by squaring the correlation between the outcome variable and Model 3's fitted values, suggests our final model accounts for 27% of the total variation in STARS scores.
  
```{r include=FALSE}
cor(data$stars_score, predict(model_3, re.form=NA))^2
```
  
  
  Of the two state-level variables, RPS target was found to be significant, $\beta=.09, \, p=.021$. This suggests that for every one percentage point increase in RPS target, there is a .09 percentage point increase in STARS rating. State legislature composition was not a significant predictor. Carnegie classifications (except Baccalaureate) and revenue continued to be significant predictors. The final model's intercept of .596 can be interpreted as the average STARS rating for a public doctoral institution in a county with 50% of Democratic voters, an average proportion of out-of state-students, average revenue, a state legislature with the average percentage of Democrats, and 0% RPS target (i.e. no RPS target).
  

```{r models, echo=FALSE, message=FALSE, warning=FALSE, tab.cap="Multi-Level Models - DV: STARS Ratings"}
coef_names <- c("(Intercept)" = "Intercept",
                "vote_pct_dem"="Vote % (Dem)",
                "carnegieMaster's" = "Carnegie: Master",
                "carnegieBaccalaureate" = "Carnegie: Bacc.",
                "carnegieOther" = "Carnegie: Other",
                "controlPrivate" = "Control (Private)",
                "revenue_mil_c" = "Revenue (in millions)",
                "out_of_state_c" = "% Out of State Stud.",
                "rps_target" = "State RPS Targets",
                "state_leg_pct_dem_c" = "State Leg. (% Dem)",
                "sd__(Intercept)"="SD Intercept (Ï„00)",
                "sd__Observation" = "SD Observation (Ïƒ2)")

msummary(list("M0: Unconditional Model" = model_0,
              "M1: Level-1 Predictors" = model_1,
              "M2: Level-1 w/covariates" = model_2,
              "M3: Level-2 Predictors" = model_3),
        coef_map = coef_names,
  stars = T,
  output = 'flextable') %>%
  flex_lme()

```

  According to @luke2017evaluating, caution should be used when evaluating the statistical significance of the fixed effects in mixed models, as there are multiple methods to do so with no consensus on which is best to use, especially under different sample sizes and model complexities. Therefore, it is also important to examine additional information, such as effect sizes. One way to measure the magnitude of an estimate's effect is through standardized coefficients. This allows interpretation to represent the result of a one standard deviation change in $X$ being associated with a $b$ standard deviation increase in $Y$. It also makes the estimates directly comparable to each other [@lorah2018effect].
  
  Standardized estimates were obtained by a complete refit of the model using standardized Z-scored data, computed using the `effectsize` 0.3.3 package in R [@effectsize]. Table \@ref(tab:model3std) displays the standardized estimates for Model 3 (see also Figure \@ref(fig:forestplot). According to these estimates, Carnegie classifications (Master's, other; -.74, -.45, respectively) and Control (-.69) have large effect sizes. These effect sizes indicate the standard deviation different in comparison to doctoral and public institutions. Out of state also has a medium effect size of .32, followed by RPS target at .19. Both vote percentage (.07) and state legislature composition have small effect sizes (.09).


```{r model3std, echo=FALSE, message=FALSE, warning=FALSE, tab.cap="Model 3 Standardized Coefficients and Confidence Intervals"}
effectsize::standardize_parameters(model_3, method="posthoc") %>%
  as.data.frame() %>%
  mutate(across(where(is.numeric),round,2)) %>%
   add_column(data.frame(Variable=c("Intercept",
              "Vote % (Dem)",
              "Carnegie: Master",
              "Carnegie: Bacc.",
              "Carnegie: Other",
              "Control (Private)",
              "Revenue (in millions)",
              "% Out-of-State Stud.",
              "State RPS Targets",
              "State Leg. (% Dem)")), 
             .before=1) %>%
  select(-CI,-Parameter) %>%
  rename("Standardized Coeff." = Std_Coefficient,
         "CI Lower" = CI_low,
         "CI Upper" = CI_high) %>%
  flex()
```

```{r forestplot, fig.cap="Forest Plot of Confidence Intervals (lines) and Standardized Estimates (dots)", message=TRUE, warning=FALSE, echo=FALSE}
sjPlot::plot_model(model_3, type="std",
                   title="Forest Plot of Confidence Intervals (lines) and Standardized Regression Coefficients (dots)")
```

  To assess the adequacy of the final model, a series of diagnostic analyses were conducted as suggested by @hoxmultilevel. Based on a scatterplot of standardized residuals plotted against fitted values, the assumption of heteroscedasticity was not violated (\@ref(fig:heterosk)). Histograms and QQ plots (Figure \@ref(fig:reshist)) were used to assess the assumption of normally distributed residuals which did not indicate a violation of normality. A scatterplot \@ref(fig:lin) of standardized residuals against STARS ratings indicated the assumption of linearity of satified. A robustness check of the model was completed by refitting the model using linear regression and then calculating clustered, robust standard errors. Model conclusions did not substantively change.
  
```{r warning=FALSE, include=FALSE}
model_lm <- lm(stars_score ~ 1 + vote_pct_dem + 
                            carnegie + control + revenue_mil_c +
                            out_of_state_c +
                            rps_target + state_leg_pct_dem_c,
                          data=data)


#robustness check
clustered <- sandwich::vcovCL(model_lm, cluster=data$state, type="HC0")

lmtest::coeftest(model_lm, vcov=clustered)
#https://data.princeton.edu/wws509/r/robust
```



```{r heterosk, fig.cap="Residuals Plotted Against Fitted Values", message=FALSE, warning=FALSE, echo=FALSE}
res_model_0 <- resid(model_0, type="pearson", scaled=T) 
res_model_0_lv2 <- ranef(model_0, condVar=T) %>%
  as.data.frame() %>%
  mutate(stand = condval/condsd) %>%
  pull(stand)
pred_model_0 <- predict(model_0)

res_model_3 <- resid(model_3, type="pearson", scaled=T) 
res_model_3_lv2 <- ranef(model_3, condVar=T) %>%
  as.data.frame() %>%
  mutate(stand = condval/condsd) %>%
  pull
pred_model_3 <- predict(model_3)

#plot(model_0)

residuals <- cbind(res=res_model_0, fit=pred_model_0) %>%
  as.data.frame() %>%
  mutate(model = "Model 0") %>%
  rownames_to_column("id") %>%
  rbind(cbind(res=res_model_3, fit=pred_model_3) %>%
  as.data.frame() %>%
  mutate(model = "Model 3") %>%
  rownames_to_column("id"))

residuals %>%
  ggplot(aes(y=fit, x=res))+
  geom_point()+
  geom_hline(yintercept = 0)+
  #geom_smooth()+
  facet_wrap(~model, scales="free")+
  theme_bw()
```

```{r reshist, fig.cap="Histogram of Residuals", message=FALSE, warning=FALSE, echo=FALSE}
residuals %>%
  ggplot(aes(x=res))+
  geom_histogram()+
  facet_wrap(~model, scales="free")+
  theme_bw()

```
```{r qq, fig.cap="QQ-Plot of Residuals", message=FALSE, warning=FALSE, echo=FALSE}
residuals %>%
  ggplot(aes(sample=res))+
  geom_qq()+
  geom_qq_line()+
  facet_wrap(~model, scales="free")+
  theme_bw()
```

```{r lin, echo=FALSE, fig.cap="Linearity between Residuals and Outcome", message=TRUE, warning=FALSE}
library(patchwork)
(cbind(data, res = res_model_0) %>%
  ggplot(aes(y=stars_score, x=res))+
  geom_point()+
  geom_smooth()+
  ggtitle("Model 0"))+
  cbind(data, res = res_model_3) %>%
  ggplot(aes(y=stars_score, x=res))+
  geom_point()+
  geom_smooth()+
  ggtitle("Model 3")
```

```{r m3lin, echo=FALSE}
data %>%
  ungroup() %>%
cbind(res_m0 = res_model_0, res_m3 = res_model_3) %>%
  select(stars_score, vote_pct_dem, revenue, out_of_state, rps_target, state_leg_pct_dem, res_m0, res_m3) %>%
  pivot_longer(stars_score:state_leg_pct_dem) %>%
  ggplot(aes(x=value, y=res_m3))+
  geom_point()+
  geom_smooth(method="loess")+
  facet_wrap(~name, scales="free")
```

# Discussion

  Findings from the analyses above allow us to draw several noteworthy conclusions about important factors associated with STARS sustainability ratings. The first research question asked whether there was a relationship between local political attitudes and institutional sustainability ratings. The results indicated that there is a small but non-significant relationship between how a county-votes and an institution's STARS rating. While there is a strong relationship between how Democratic a county votes and its belief in anthropogenic climate change (see Figure \@ref(fig:cor_dem_cc)), when considering institutional controls and state-level factors, this association has little effect on STARS ratings. This was somewhat surprising, as partisan affiliation has a strong relationship to climate change beliefs [@hess2019climate; jenkins2020partisan]. These findings indicate that local-level political beliefs may only have minimal bearing *en masse* on the sustainability of an institution. However, as will be argued below, individual stakeholders from the local level may, in fact, still play an important role in institutional sustainability efforts.
  
  Research question two sought to measure the relationship between local environmental beliefs and institutional sustainability ratings. Climate change was the key proxy for these beliefs. However, due to collinearity issues with vote percentage, climate change beliefs were removed from the model and thus research question two is answered, indirectly by research question one.^[Models were estimated that included climate change beliefs and results for climate change were similar to that of county vote percentage.] STARS, of course, is not solely related to climate change beliefs. It is composed of measures for energy, academics, and outreach among other areas. Climate change is certainly a concern that STARS seeks to measure, but it is not necessarily central to its ratings. There are, therefore, likely other mechanisms that mediate the relationship between partisan ideology and STARS ratings. 
  
  Furthermore, how local political affiliation affects institutional policies and actions related to sustainability varies. Local laws may govern building practices, composting, or energy generation. These laws, however, may be tied more to resources, geography, or other reasons besides political affiliation. Furthermore, the relationship between community colleges and local politics is closer and more direct than between larger institutions, especially those overseen by a state board [@heilocal]. It is likely that the majority of STARS-rated institutions have stronger influences at the state than at the local level.
  
  Most (94%) of the variation of STARS ratings was found to be due to in-state variation, and most of that variation was related to institutional characteristics. However, there was an appreciable amount of variation between states that was measured. In fact, of the 6% of variability found to be due to differences between states, state RPS target and state legislature composition accounted for a vast majority (90%) of that variation in addition to level-1 institutional characteristics.
  
  The third research question inquired about the relationship between state political composition and STARS ratings. The results indicated that state legislative composition was a non-significant predictor of STARS ratings, with only a small effect ($b=.09$). This finding is similar to that of county-level vote percentage. It suggests that the partisan makeup of state legislatures may not directly affect institutional environmental sustainability to a major extent.
  
  To answer the fourth research question, state RPS targets were used to determine whether there was a relationship between state environmental policies and institutional sustainability ratings. A significant relationship was indeed found: a one-percentage point increase in RPS target is associated with an .09 percentage point increase in STARS ratings, a small effect ($b=.19$) compared to other variables in Model 3. Institutions in states which have renewable energy goals are more likely to have higher STARS ratings. This may be a result of institutions reflecting the sustainability ambitions of their state. Because RPS targets are state-level policies, higher STARS ratings in these states may also be a result of state-level sustainability policies.
  
  Research by @berryRPS has suggested that Democratically-controlled legislatures have a major role in setting or amending RPS targets. While RPS targets were a significant predictor of STARS ratings, legislative composition was not. This is somewhat surprising given the research. One reason for this somewhat contradictory finding could be that state sustainability policies are merely one influence on STARS ratings and a myriad of other sources of influence exist at the institution-level. Another reason is that RPS policies are not driven solely by partisanship. @berryRPS also found that RPS policies are driven by the price of electricity, existing renewable energy infrastructure, a state's median income, and even its potential for wind energy. These complex characteristics of RPS targets may explain the importance of RPS targets in our model and why the legislative composition was not.
  
  The largest effects on STARS ratings seem to exist at the institution-level. In particular, the type of institution (i.e., Carnegie classification) were the biggest factor in STARS ratings, with Doctoral institutions having the highest ratings. Doctoral institutions tend to be the largest institutions (with average enrollment sizes of 20,000 and above). This affords Doctoral institutions with a larger pool of internal stakeholders (e.g., students, faculty, staff) for whom sustainability is important. This larger size increases the chance for individual champions who may lead campus sustainability [@shriberg2002sustainability]. @shriberg2002sustainability specifically pointed to faculty as a major driver of campus sustainability. As Doctoral institutions tend to have a larger faculty population with high research activity, the possibility that faculty may influence sustainability increases.
  
  Beyond internal institutional stakeholders, Doctoral institutions, with their large student bodies, may also enjoy better financial health. According to Arthur B. Sacks, professor emeritus of Environmental Studies at the Colorado School of Mines, "the financial health of institutions and the communities in which they exist plays an important part in whether they support and promote sustainability" (personal communication, September 15, 2020). Essentially, if an institution can afford the cost of, for example, green construction, revamping the curriculum to include sustainability, or having a sustainability office at all, then they are more likely to do so. In fact, financial health may explain why smaller, financially well-off, institutions, such as baccalauerate colleges (e.g., liberal arts colleges), also tend to have higher STARS ratings. These institutions tend to be private, not-for-profit institutions with larger financial means compared to associate's (i.e. community college) institutions or regional public colleges. Likewise, public institutions, who also have statistically significantly larger STARS ratings may enjoy greater financial health due to state subsidies and their larger institutional sizes. This would also explain why revenue and out-of-state students (who often pay higher tuition) are significant predictors of STARS ratings. Given this finding, there was some concern for a strong correlation between revenue and percentage of out-of-state students; however, none was found ($r=.08,  \, p=.094$).
  
```{r eval=FALSE, include=FALSE}
data %>% 
  ungroup() %>%
  select(revenue_mil, out_of_state) %>% 
  cortest()
```
  
  Viewing the findings through stakeholder theory, it was originally hypothesized that local communities and state-level governmental stakeholders may have a major role in influencing higher education sustainability ratings and that this influence may fall along partisan lines. The local community, being made up of faculty and staff as well as non-institutionally-related citizens, by way of their political voice, could drive local policies to be inclusive or focused on sustainability issues. In so far as this could be measured by partisan vote, this was found to have little, if any, influence, consigning the local community to peripheral stakeholders.
  
  Stakeholder theory in higher education has stronger support for the government as a stakeholder, both affecting and being affected by higher education in a number of ways. On the one hand, the present research did not find a significant role for the government as a stakeholder in sustainability as measured by legislative partisan composition. On the other hand, state RPS targets were found to influence STARS rating, and, as these RPS targets are a function, to some degree, of government, a stakeholder role for government is therefore evident, though not primary. Indeed, as institutional characteristics were found to be the most predictive of STARS ratings, this confirms the major stakeholders are those who champion and ultimately decide sustainability policies on campus: the students, faculty, and staff that make up the institutions themselves.


# Limitations

  At least three limitations should be considered with interpreting the findings of this study. First, while a sample size of 363 institutions and 46 states plus DC would generally not be considered a small data set, this study included several states with a single institution. The inclusion of upper-level units (i.e., states) with only a single institution hindered our ability to use different multilevel modeling features ([@hoxmultilevel]). For instance, due to the small within-cluster sample size, a random coefficient model could not be estimated. Such a model may better fit the data and allow for a more detailed analysis of the characteristics that impact STARS ratings. Likewise, within-cluster (i.e., group-mean) centering was also not feasible. 
  
  The obvious solution to this would be to increase the sample size. However, the entire population of STARS-rated institutions was included in the analysis. As more institutions undertake STARS ratings, the dataset is likely to grow. A possible solution to the single institution problem would be to combine states into regions and estimate within- and between-region effects, but such a model requires different theoretical assumptions about how a region can affect political ideology or sustainability targets.
  
  Another limitation is the lack of consideration of state appropriations in both the theoretical and statistical model. The current model considers both private and public institutions, and thus only includes revenue based on tuitition and fees. Were this model to consider only public institutions, state appropriations could also be used as a predictor of STARS ratings. In partiular, appropriations targeted for sustainability efforts or building projects with sustainability designs (e.g., LEED certification) may also be important considerations. A model such as this, with only public institutions, would, by its nature be smaller and suffer from some of the same limitations as discussed above.
  
  A final limitation is that the dependent variable is comprised of overall STARS ratings of institutions, and that some of these institutions are rated on different versions of STARS (e.g. 2.2, 2.1., 2.0). Although each version of STARS utilized the same scale to score institutions, the reader is advised to consider this limitation when interpreting the findings. Once more institutions have been rated using the most up-to-date version, future research should focus only on that version. However, in terms of understanding  sustainability of an institution, the overall score from any version offers a clear picture of an institution's sustainability efforts and, because they fall on the same 0-100 percentage scale, allow them to be estimated together. Despite these limitations, this study still offers valuable information regarding the factors that predict STARS ratings. 

# Implications

  The findings of this research suggest several implications. Because this research reaffirms the notion that institution-level stakeholders (i.e., faculty, students, staff) may play a major role in campus sustainability, it is therefore imperative to seek out relationships with such stakeholders. These relationships can take the form of faculty who can (or already do) integrate sustainability into the curriculum. It could include building relationships or fostering interrelationships among faculty and research staff who focus on various aspects of sustainability, which can lead to curricular integration, the development of local projects (such as GHG offset projects), and other technology-based sustainability solutions. Furthermore, working with student environmental groups and interested individuals can lead to increased sustainability initiatives and innovative programming.
  
  The findings also make it clear that sustainability requires money. This money can come from state subsidies or earmarked from other revenue sources. Another idea is to institute what many institutions call a "green fee". A 2008 report from the University of Michigan found that green fees range from \$1 to \$20 per student per semester and can raise between \$10,000 and $200,000 a year or more depending on fee amount and number of students [@greenfee]. A survey of 65 institutions with green fees indicated that the fees typically pay for campus infrastructure projects, on-campus education/awareness campaigns, research, offset purchases, and off-campus projects [@bintliff].
  
  In terms of political implications, suggestions are less clear as changing partisan voting behavior is quite difficult. This research suggests that partisan behavior may affect sustainability indirectly through RPS targets and their influence on higher education institutions. It may be useful to raise awareness of sustainability issues at the local level in order to promote the selection of candidates who can institute or increase RPS targets at the state level. In addition, selecting representatives who can lobby for state, and federal, higher education financial investment, especially for sustainability, can help increase or direct funds to needed sustainability initiatives.

# Conclusion

  The planet is hurtling towards a human-caused global warming of 1.5 degrees Celsius over pre-industrial levels, a tiny number that serves as a demarcation line for climate change leading to numerous ecological and humanitarian crises [@clark2020future; @ipcc]. Governments have sought to address this crisis through curbing emissions, renewable technology investments, and participation in international accords (e.g. the Kyoto Protocol, Paris Climate Agreement). Higher education institutions have also been called upon to tap their research and educational potentials while serving as a role model for society, joining such agreements as the American College and University Presidents Climate Commitment.
  
  Concomitant with the rise in higher education sustainability initiatives have been the rise of HESAs, serving as tools for benchmarking, self-assessment, and comparisons. The STARS assessment is the most ubiquitous HESA in North American. The purpose of the present research was to investigate what factors are associated with higher STARS ratings. Stakeholder theory was employed to understand who affects and is affected by higher education sustainability and therefore may play a role in such ratings. Of particular interest were the local community influence (measured through partisan voting behavior) and state government influence (measured through state legislative composition and RPS targets). The results indicated that measures of local partisanship have little to no influence while RPS targets have a small effect on STARS ratings. Furthermore, the results reaffirmed past research that indicated institution-level stakeholders (e.g., faculty, students, staff) and finances play a larger and more critical role in institutional sustainability.
  

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
